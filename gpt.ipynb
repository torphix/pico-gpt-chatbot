{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/Programming/DeepLearning/picoGPT/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: data/jre/#1422.txt\n",
      "Vocabulary size is: 49\n",
      "Vocabulary is: [' ', '$', '%', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<PAD>', '<START>', '<END>']\n"
     ]
    }
   ],
   "source": [
    "# Global variables: Essentially the control panel for the notebook\n",
    "DATASET_PATH = \"data/jre/#1422.txt\"  # should be a path to a txt file or a directory\n",
    "MAX_SEQ_LEN = 5\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset_path):\n",
    "    # check if dataset is a directory or a file\n",
    "    # if it is a directory, then we will concatenate all the files in the directory into one file\n",
    "    if dataset_path.endswith(\".txt\"):\n",
    "        return dataset_path\n",
    "    else:\n",
    "        for file in os.listdir(dataset_path):\n",
    "            with open(f\"{dataset_path}/{file}\", \"r\") as f:\n",
    "                with open(f\"{dataset_path}/dataset.txt\", \"a\") as f2:\n",
    "                    f2.write(\"\\n\" + f.read())\n",
    "        return f\"{dataset_path}/dataset.txt\"\n",
    "\n",
    "\n",
    "def init_charachter_based_vocab():\n",
    "    # Instantiate the vocabulary\n",
    "    with open(DATASET_PATH, \"r\") as f:\n",
    "        vocab = sorted(set(f.read().replace(\"\\n\", \"\").lower()))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def init_word_based_vocab():\n",
    "    # Instantiate the vocabulary\n",
    "    with open(DATASET_PATH, \"r\") as f:\n",
    "        vocab = sorted(\n",
    "            set(\n",
    "                f.read()\n",
    "                .replace(\"\\n\", \" \")\n",
    "                .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "                .lower()\n",
    "                .split(\" \")\n",
    "            )\n",
    "        )\n",
    "    return vocab\n",
    "\n",
    "\n",
    "DATASET_PATH = preprocess_dataset(DATASET_PATH)\n",
    "VOCAB = init_charachter_based_vocab()\n",
    "VOCAB += [\"<PAD>\"]\n",
    "VOCAB += [\"<START>\"]\n",
    "VOCAB += [\"<END>\"]\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "\n",
    "def decode_character_sequence(sequence: list):\n",
    "    return \"\".join([VOCAB[i] for i in sequence])\n",
    "\n",
    "\n",
    "def decode_word_sequence(sequence: list):\n",
    "    return \" \".join([VOCAB[i] for i in sequence])\n",
    "\n",
    "\n",
    "def encode_sequence(sequence: list):\n",
    "    return [VOCAB.index(c) for c in sequence]\n",
    "\n",
    "\n",
    "# Logs\n",
    "print(f\"Using dataset: {DATASET_PATH}\")\n",
    "print(\"Vocabulary size is:\", VOCAB_SIZE)\n",
    "print(\"Vocabulary is:\", VOCAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data\n",
    "Inputs: are arbitrary sequences of length MAX_SEQ_LEN samples from the dataset\n",
    "Targets: are the same sequences shifted by one character to the right ie: one character in the future\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class UnsupervisedCharacterDataset(Dataset):\n",
    "    def __init__(self, dataset_path: str, seq_len: int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        with open(dataset_path, \"r\") as f:\n",
    "            self.data = f.read().replace(\"\\n\", \" \").lower()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total possible number of samples is every\n",
    "        sequence length subset possible in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        \"\"\"\n",
    "        Returns a randomly samples sequence of length seq_len (input)\n",
    "        and then a target sequence which is the same sequence but shifted\n",
    "        so it is one token ahead of the input sequence\n",
    "        The model therefore learns to predict the next character\n",
    "        \"\"\"\n",
    "        x = self.data[i : self.seq_len + i]\n",
    "        y = self.data[i + 1 : self.seq_len + i + 1]\n",
    "        # Tokenize\n",
    "        x = encode_sequence([i for i in x])\n",
    "        y = encode_sequence([i for i in y])\n",
    "        return {\n",
    "            \"inputs\": torch.tensor(x),\n",
    "            \"targets\": torch.tensor(y),\n",
    "        }\n",
    "\n",
    "\n",
    "class UnsupervisedWordDataset(Dataset):\n",
    "    def __init__(self, dataset_path: str, seq_len: int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        with open(dataset_path, \"r\") as f:\n",
    "            self.data = (\n",
    "                f.read()\n",
    "                .replace(\"\\n\", \" \")\n",
    "                .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "                .lower()\n",
    "                .split(\" \")\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total possible number of samples is every\n",
    "        sequence length subset possible in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        \"\"\"\n",
    "        Returns a randomly samples sequence of length seq_len (input)\n",
    "        and then a target sequence which is the same sequence but shifted\n",
    "        so it is one token ahead of the input sequence\n",
    "        The model therefore learns to predict the next character\n",
    "        \"\"\"\n",
    "        x = self.data[i : self.seq_len + i]\n",
    "        y = self.data[i + 1 : self.seq_len + i + 1]\n",
    "        # Tokenize\n",
    "        x = encode_sequence(x)\n",
    "        y = encode_sequence(y)\n",
    "        return {\n",
    "            \"inputs\": torch.tensor(x),\n",
    "            \"targets\": torch.tensor(y),\n",
    "        }\n",
    "\n",
    "\n",
    "def load_dataloader(\n",
    "    dataset_path: str, seq_len: int, batch_size: int, character_based: bool = True\n",
    "):\n",
    "    if character_based:\n",
    "        dataset = UnsupervisedCharacterDataset(dataset_path, seq_len)\n",
    "    else:\n",
    "        dataset = UnsupervisedWordDataset(dataset_path, seq_len)\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def train_test_split(dataset_path: str, train_size: float):\n",
    "    assert train_size <= 1.0, \"Train size cannot be larger than 1\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        dataset = f.read()\n",
    "    train = dataset[: int(train_size * len(dataset))]\n",
    "    test = dataset[int(train_size * len(dataset)) :]\n",
    "    dataset_filename = dataset_path.split(\"/\")[-1]\n",
    "    dataset_root_path = \"/\".join(dataset_path.split(\"/\")[:-1])\n",
    "    with open(f\"{dataset_root_path}/train_{dataset_filename}\", \"w\") as f:\n",
    "        f.write(train)\n",
    "    with open(f\"{dataset_root_path}/test_{dataset_filename}\", \"w\") as f:\n",
    "        f.write(test)\n",
    "    print(\n",
    "        f\"Dataset split and written to: {dataset_root_path}/test_{dataset_filename} & {dataset_root_path}/train_{dataset_filename}\"\n",
    "    )\n",
    "    return (\n",
    "        f\"{dataset_root_path}/train_{dataset_filename}\",\n",
    "        f\"{dataset_root_path}/test_{dataset_filename}\",\n",
    "    )\n",
    "\n",
    "\n",
    "def test_dataloader():\n",
    "    train, test = train_test_split(DATASET_PATH, 0.9)\n",
    "    train_dataset = load_dataloader(train, MAX_SEQ_LEN, 1)\n",
    "    test_dataset = load_dataloader(test, MAX_SEQ_LEN, 1)\n",
    "    batch = next(iter(train_dataset))\n",
    "    x, y = batch[\"inputs\"], batch[\"targets\"]\n",
    "    assert torch.all(\n",
    "        x[:, 1:].eq(y[:, :-1])\n",
    "    ), \"Target values are not right shifted by one token!\"\n",
    "    print(\"Dataset is correct!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Transformer architecture: https://arxiv.org/abs/1706.03762\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, n_heads, vocab_size, emb_dim, n_blocks) -> None:\n",
    "        super().__init__()\n",
    "        self.text_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[EncoderBlock(emb_dim, n_heads) for i in range(n_blocks)]\n",
    "        )\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [DecoderBlock(emb_dim, n_heads) for i in range(n_blocks)]\n",
    "        )\n",
    "        self.out_fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        enc_x = self.text_embedding(encoder_input)\n",
    "        dec_x = self.text_embedding(decoder_input)\n",
    "        enc_x = enc_x + self.positional_embedding(encoder_input)\n",
    "        dec_x = dec_x + self.positional_embedding(decoder_input)\n",
    "        encoder_output = self.encoder(enc_x)\n",
    "        for block in self.decoder:\n",
    "            dec_x = block(encoder_output, dec_x)\n",
    "        dec_x = self.out_fc(dec_x)\n",
    "        return dec_x\n",
    "\n",
    "    def generate(self, prompt: str, max_len: int):\n",
    "        \"\"\"\n",
    "        Used for inference\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        prompt = torch.tensor([[encode_sequence(prompt)]]).to(device)\n",
    "        while i < max_len:\n",
    "            generated_output = self.forward(prompt)\n",
    "            # Select the most probable token\n",
    "            prompt = torch.cat(\n",
    "                (prompt, generated_output.argmax(dim=-1).unsqueeze(0)), dim=1\n",
    "            )\n",
    "            i += 1\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, in_d: int, out_d: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Parralelise the attention heads\n",
    "        self.W_q = nn.Linear(in_d, in_d * n_heads, bias=False)\n",
    "        self.W_k = nn.Linear(in_d, in_d * n_heads, bias=False)\n",
    "        self.W_v = nn.Linear(in_d, in_d * n_heads, bias=False)\n",
    "        self.fc_out = nn.Linear(in_d * n_heads, out_d, bias=True)\n",
    "\n",
    "    def softmax_attention(self, q, k, v, mask=None):\n",
    "        # Caculate dot product\n",
    "        qk = q @ k.transpose(-2, -1)\n",
    "        # Normalise by the square root of the dimension\n",
    "        qk = qk / math.sqrt(q.shape[-1])\n",
    "        # Apply mask if decoder\n",
    "        if mask is not None:\n",
    "            qk = qk.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        # Calculate attention scores\n",
    "        qk = F.softmax(qk, dim=-1)\n",
    "        # Apply attention scores to values\n",
    "        out = qk @ v\n",
    "        return out\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # [batch_size, seq_len, emb_dim] -> [batch_size, n_heads, seq_len, emb_dim/n_heads]\n",
    "        batch_size = x.shape[0]\n",
    "        return (\n",
    "            x.reshape(batch_size, -1, self.n_heads, x.shape[-1] // self.n_heads)\n",
    "            .permute(0, 2, 1, 3)\n",
    "            .reshape(batch_size * self.n_heads, -1, x.shape[-1] // self.n_heads)\n",
    "        )\n",
    "\n",
    "    def concat_heads(self, x):\n",
    "        # [batch_size * n_heads, seq_len, emb_dim/n_heads] -> [batch_size, seq_len, emb_dim*heads]\n",
    "        batch_size = x.shape[0] // self.n_heads\n",
    "        return (\n",
    "            x.reshape(batch_size, self.n_heads, -1, x.shape[-1])\n",
    "            .permute(0, 2, 1, 3)\n",
    "            .reshape(batch_size, -1, x.shape[-1] * self.n_heads)\n",
    "        )\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Embed the input [batch_size, seq_len, emb_dim] -> [batch_size, seq_len, n_heads, emb_dim/n_heads]\n",
    "        q = self.split_heads(self.W_q(q))\n",
    "        k = self.split_heads(self.W_k(k))\n",
    "        v = self.split_heads(self.W_v(v))\n",
    "        # Calculate attention\n",
    "        attention_score = self.softmax_attention(q, k, v, mask)\n",
    "        # Concatenate the heads\n",
    "        out = self.concat_heads(attention_score)\n",
    "        # Apply final linear layer\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, in_d, out_d) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_d, out_d),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_d, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadedAttention(n_heads, in_d, in_d)\n",
    "        # Note: FF block is sandwhiched between layer norms as res conenction occurs here\n",
    "        self.ff = FeedForwardBlock(in_d, in_d)\n",
    "        self.norm_1 = nn.LayerNorm(in_d)\n",
    "        self.norm_2 = nn.LayerNorm(in_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.mha(x, x, x)\n",
    "        x = self.norm_1(x + res)\n",
    "        res = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm_2(x + res)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_d, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        # Decoder only\n",
    "        self.masked_mha = MultiHeadedAttention(n_heads, in_d, in_d)\n",
    "        # For encoder and decoder\n",
    "        self.cross_mha = MultiHeadedAttention(n_heads, in_d, in_d)\n",
    "        # Note: FF block is sandwhiched between layer norms as res conenction occurs here\n",
    "        self.ff = FeedForwardBlock(in_d, in_d)\n",
    "        self.norm_1 = nn.LayerNorm(in_d)\n",
    "        self.norm_2 = nn.LayerNorm(in_d)\n",
    "\n",
    "    def make_mask(self, dec_x):\n",
    "        # Create a mask for the decoder to prevent it from looking ahead\n",
    "        mask = torch.tril(torch.ones(dec_x.shape[1], dec_x.shape[1]), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        return mask.unsqueeze(0).to(dec_x.device)\n",
    "\n",
    "    def forward(self, enc_x, dec_x):\n",
    "        # Layer 1\n",
    "        res = dec_x\n",
    "        mask = self.make_mask(dec_x)\n",
    "        dec_x = self.masked_mha(dec_x, dec_x, dec_x, mask)\n",
    "        dec_x = self.norm_1(dec_x + res)\n",
    "        # Layer 2\n",
    "        res = dec_x\n",
    "        x = self.cross_mha(dec_x, enc_x, enc_x)\n",
    "        x = self.norm_1(x + res)\n",
    "        # Layer 3\n",
    "        res = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm_2(x + res)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Transformer architecture: https://arxiv.org/abs/1706.03762\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GPTDecoderTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a decoder only transformer for next token prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, vocab_size, emb_dim, n_blocks) -> None:\n",
    "        super().__init__()\n",
    "        self.text_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [GPTDecoderBlock(emb_dim, n_heads) for i in range(n_blocks)]\n",
    "        )\n",
    "        self.out_fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_emb = self.text_embedding(x)\n",
    "        x = x_emb + self.positional_embedding(x)\n",
    "        for block in self.decoder:\n",
    "            x = block(x)\n",
    "        x = self.out_fc(x)\n",
    "        return x\n",
    "\n",
    "    def generate(self, prompt: str, max_len: int):\n",
    "        \"\"\"\n",
    "        Used for inference\n",
    "        \"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        prompt = torch.tensor([encode_sequence(prompt)]).to(device)\n",
    "        while prompt.shape[-1] < max_len:\n",
    "            generated_output = self.forward(prompt)\n",
    "            # Select the most probable token\n",
    "            output = F.softmax(generated_output[:,-1,:], dim=-1)\n",
    "            next_token = torch.multinomial(output, num_samples=1)\n",
    "            prompt = torch.cat((prompt, next_token), dim=1)\n",
    "        output = prompt\n",
    "        return output\n",
    "\n",
    "\n",
    "class GPTDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_d, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        # Decoder only\n",
    "        self.masked_mha = MultiHeadedAttention(n_heads, in_d, in_d)\n",
    "        # For encoder and decoder\n",
    "        self.cross_mha = MultiHeadedAttention(n_heads, in_d, in_d)\n",
    "        # Note: FF block is sandwhiched between layer norms as res conenction occurs here\n",
    "        self.ff = FeedForwardBlock(in_d, in_d)\n",
    "        self.norm_1 = nn.LayerNorm(in_d)\n",
    "        self.norm_2 = nn.LayerNorm(in_d)\n",
    "\n",
    "    def make_mask(self, dec_x):\n",
    "        # Create a mask for the decoder to prevent it from looking ahead\n",
    "        mask = torch.tril(torch.ones(dec_x.shape[1], dec_x.shape[1]))\n",
    "        return mask.unsqueeze(0).to(dec_x.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        res = x\n",
    "        mask = self.make_mask(x)\n",
    "        x = self.masked_mha(x, x, x, mask)\n",
    "        x = self.norm_1(x + res)\n",
    "        # Layer 2\n",
    "        res = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm_2(x + res)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_dl: DataLoader,\n",
    "        val_dl: DataLoader,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        character_based: bool = False,\n",
    "    ) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.character_based = character_based\n",
    "\n",
    "    def run(self, epochs: int):\n",
    "        self.model.train()\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self.train_step()\n",
    "            val_loss, output = self.val_step()\n",
    "        # Save model\n",
    "        torch.save(self.model.state_dict(), \"model.pt\")\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Performs one pass over the training data\n",
    "        \"\"\"\n",
    "        train_loss = 0\n",
    "        self.model.train()\n",
    "        for batch in tqdm(self.train_dl, desc=f\"Train Step\", leave=False):\n",
    "            x, y = batch[\"inputs\"], batch[\"targets\"]\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(x)\n",
    "            loss: torch.Tensor = self.loss_fn(out.permute(0, 2, 1), y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_loss /= 2\n",
    "        return train_loss\n",
    "\n",
    "    def val_step(self):\n",
    "        \"\"\"\n",
    "        Performs one pass over the validation set\n",
    "        \"\"\"\n",
    "        val_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_dl, desc=\"Val Step\", leave=False):\n",
    "                x, y = batch[\"inputs\"], batch[\"targets\"]\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                out = self.model(x)\n",
    "                loss: torch.Tensor = self.loss_fn(out.permute(0, 2, 1), y)\n",
    "                val_loss += loss.item()\n",
    "                return val_loss, out\n",
    "            val_loss /= len(self.val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split and written to: data/jre/test_#1422.txt & data/jre/train_#1422.txt\n",
      "Starting training for 100 epochs...\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Step:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = GPTDecoderTransformer(n_heads=4, vocab_size=len(VOCAB), emb_dim=256, n_blocks=6)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "train_path, val_path = train_test_split(DATASET_PATH, 0.99)\n",
    "train_dl, val_dl = load_dataloader(\n",
    "    \"/home/j/Desktop/Programming/DeepLearning/picoGPT/data/jre/dataset.txt\",\n",
    "    128,\n",
    "    8,\n",
    "    character_based=True,\n",
    "), load_dataloader(\n",
    "    \"/home/j/Desktop/Programming/DeepLearning/picoGPT/data/jre/dataset.txt\",\n",
    "    128,\n",
    "    8,\n",
    "    character_based=True,\n",
    ")\n",
    "Trainer(model, train_dl, val_dl, loss_fn, optimizer, True).run(epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model \u001b[39m=\u001b[39m GPTDecoderTransformer(n_heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(VOCAB), emb_dim\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, n_blocks\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[1;32m     19\u001b[0m predictor \u001b[39m=\u001b[39m Predictor(model, \u001b[39m\"\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m predictor\u001b[39m.\u001b[39;49mlaunch_interactive()\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36mPredictor.launch_interactive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter a prompt: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(prompt))\n",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(prompt, \u001b[39m1024\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(decode_character_sequence(output[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()))\n",
      "Cell \u001b[0;32mIn[15], line 42\u001b[0m, in \u001b[0;36mGPTDecoderTransformer.generate\u001b[0;34m(self, prompt, max_len)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39m# Select the most probable token\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(generated_output[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     next_token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(output, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     43\u001b[0m     prompt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((prompt, next_token), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m output \u001b[39m=\u001b[39m prompt\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model: nn.Module, pth_path: str) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        self.model.load_state_dict(torch.load(pth_path))\n",
    "\n",
    "    def predict(self, prompt: str):\n",
    "        output = self.model.generate(prompt, 1024)\n",
    "        print(decode_character_sequence(output[0].tolist()))\n",
    "\n",
    "    def launch_interactive(self):\n",
    "        while True:\n",
    "            prompt = input(\"Enter a prompt: \")\n",
    "            print(self.predict(prompt))\n",
    "\n",
    "\n",
    "model = GPTDecoderTransformer(n_heads=4, vocab_size=len(VOCAB), emb_dim=256, n_blocks=6)\n",
    "predictor = Predictor(model, \"model.pt\")\n",
    "predictor.launch_interactive()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66e58e144739996868149dbe6413ac7a12839919439ff5ebb9e75c32680b3c30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
