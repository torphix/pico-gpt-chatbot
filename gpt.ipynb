{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/Programming/DeepLearning/picoGPT/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: ./test.wav.txt\n",
      "Vocabulary size is: 478\n",
      "Vocabulary is: ['', 'a', 'ability', 'about', 'absolutely', 'absurd', 'absurdism', 'absurdities', 'absurdity', 'academically', 'accentuate', 'accept', 'actions', 'after', 'again', 'ago', 'albert', 'all', 'also', 'am', 'an', 'and', 'angle', 'angry', 'another', 'answer', 'anything', 'anyway', 'apart', 'are', 'around', 'as', 'aspects', 'associated', 'at', 'away', 'bakunin', 'basically', 'be', 'beautiful', 'beauty', 'because', 'been', 'beethovens', 'being', 'beings', 'believe', 'best', 'bit', 'bob', 'book', 'both', 'bother', 'brakes', 'brew', 'brilliance', 'bring', 'bringing', 'broad', 'build', 'built', 'but', 'by', 'call', 'called', 'camus', 'can', 'cant', 'car', 'care', 'categories', 'cause', 'caymus', 'characterize', 'child', 'circumstances', 'circunstancia', 'circus', 'clear', 'come', 'comes', 'comment', 'compare', 'concisely', 'conflate', 'connection', 'consistent', 'contentment', 'context', 'control', 'corner', 'cosmic', 'could', 'counterproductive', 'create', 'created', 'creative', 'daytoday', 'death', 'deciding', 'decisions', 'define', 'definitions', 'described', 'differences', 'different', 'difficult', 'disagree', 'discover', 'discovered', 'discovering', 'discovery', 'distress', 'do', 'doesnt', 'doing', 'dont', 'doug', 'eastern', 'either', 'embracing', 'en', 'end', 'ends', 'enjoy', 'even', 'ever', 'everything', 'except', 'existence', 'existentialism', 'existentialist', 'existentialists', 'existentially', 'experience', 'experienced', 'experiences', 'explain', 'faith', 'feelings', 'felt', 'few', 'first', 'follow', 'for', 'foster', 'from', 'front', 'fully', 'fundamental', 'gasset', 'generate', 'genetic', 'get', 'getting', 'ghost', 'give', 'given', 'go', 'going', 'gonna', 'good', 'gratitude', 'ground', 'guide', 'guy', 'had', 'hand', 'happen', 'happening', 'happens', 'have', 'having', 'he', 'health', 'healthy', 'hear', 'heaviness', 'hell', 'help', 'helps', 'here', 'heres', 'him', 'his', 'how', 'huh', 'human', 'humans', 'humility', 'humor', 'i', 'id', 'idea', 'ideas', 'if', 'important', 'in', 'indepth', 'individual', 'inside', 'instead', 'intellectual', 'intended', 'interesting', 'internet', 'into', 'is', 'isnt', 'it', 'its', 'jaded', 'joy', 'just', 'kafkaesque', 'kierkegaard', 'kind', 'know', 'knowing', 'lands', 'later', 'laugh', 'learn', 'learning', 'least', 'left', 'less', 'let', 'lets', 'level', 'lies', 'life', 'like', 'liking', 'little', 'live', 'living', 'look', 'looking', 'lose', 'lost', 'lot', 'love', 'magic', 'magical', 'make', 'makes', 'man', 'many', 'marvel', 'matter', 'matters', 'maybe', 'mckenzie', 'me', 'mean', 'meaning', 'memes', 'mi', 'middle', 'mindset', 'minutes', 'moment', 'moments', 'more', 'most', 'movie', 'my', 'myself', 'myth', 'nature', 'negative', 'new', 'nihilism', 'nihilistic', 'nihilists', 'ninth', 'no', 'nonattachment', 'not', 'nothing', 'notice', 'now', 'nuanced', 'number', 'observing', 'of', 'often', 'oh', 'okay', 'on', 'one', 'only', 'operates', 'or', 'ortega', 'other', 'others', 'our', 'ourselves', 'out', 'outside', 'over', 'overall', 'overarching', 'overly', 'own', 'part', 'passion', 'people', 'persecution', 'person', 'perspective', 'perspectives', 'philosophical', 'philosophy', 'place', 'playing', 'points', 'positive', 'pretend', 'pretty', 'problem', 'proscribed', 'publish', 'published', 'purpose', 'pursuing', 'question', 'questions', 'quote', 'railing', 'real', 'really', 'reduce', 'reductionist', 'reject', 'relation', 'religious', 'remember', 'rid', 'right', 'sad', 'sadness', 'said', 'sartre', 'saw', 'say', 'saying', 'says', 'scary', 'search', 'see', 'seeing', 'sense', 'serious', 'shades', 'short', 'should', 'simple', 'simplicity', 'sisyphus', 'slippery', 'slope', 'smile', 'so', 'social', 'society', 'some', 'somehow', 'someone', 'something', 'somethings', 'somewhere', 'sort', 'soy', 'specifics', 'spectrum', 'spinoza', 'start', 'started', 'statement', 'steering', 'step', 'stories', 'story', 'strange', 'structures', 'suicide', 'symphony', 'sync', 'system', 'systems', 'take', 'takes', 'talk', 'talking', 'tease', 'tell', 'tend', 'terms', 'than', 'that', 'thats', 'the', 'them', 'themselves', 'then', 'there', 'theres', 'these', 'they', 'thick', 'thing', 'things', 'think', 'this', 'those', 'thought', 'ties', 'to', 'together', 'too', 'totally', 'trips', 'trombone', 'truth', 'try', 'trying', 'two', 'tying', 'ultimately', 'understood', 'uninspired', 'up', 'us', 'very', 'wait', 'walk', 'wall', 'want', 'was', 'way', 'we', 'well', 'were', 'western', 'weve', 'what', 'when', 'where', 'whether', 'which', 'who', 'why', 'wisdom', 'with', 'within', 'work', 'world', 'worth', 'would', 'writes', 'writing', 'writings', 'wrong', 'wrote', 'y', 'yeah', 'yes', 'yo', 'you', 'younger', 'your', 'youre', '<PAD>', '<START>', '<END>']\n"
     ]
    }
   ],
   "source": [
    "# Global variables: Essentially the control panel for the notebook\n",
    "DATASET_PATH = \"./test.wav.txt\"  # should be a path to a txt file\n",
    "MAX_SEQ_LEN = 5\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def init_charachter_based_vocab():\n",
    "    # Instantiate the vocabulary\n",
    "    with open(DATASET_PATH, \"r\") as f:\n",
    "        vocab = sorted(set(f.read().replace(\"\\n\", \"\").lower()))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def init_word_based_vocab():\n",
    "    # Instantiate the vocabulary\n",
    "    with open(DATASET_PATH, \"r\") as f:\n",
    "        vocab = sorted(\n",
    "            set(\n",
    "                f.read()\n",
    "                .replace(\"\\n\", \" \")\n",
    "                .translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "                .lower()\n",
    "                .split(\" \")\n",
    "            )\n",
    "        )\n",
    "    return vocab\n",
    "\n",
    "\n",
    "VOCAB = init_word_based_vocab()\n",
    "VOCAB += [\"<PAD>\"]\n",
    "VOCAB += [\"<START>\"]\n",
    "VOCAB += [\"<END>\"]\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "\n",
    "def decode_character_sequence(sequence:list):\n",
    "    return \"\".join([VOCAB[i] for i in sequence])\n",
    "\n",
    "def decode_word_sequence(sequence:list):\n",
    "    return \" \".join([VOCAB[i] for i in sequence])\n",
    "\n",
    "def encode_sequence(sequence:list):\n",
    "    return [VOCAB.index(c) for c in sequence]\n",
    "\n",
    "\n",
    "# Logs\n",
    "print(f\"Using dataset: {DATASET_PATH}\")\n",
    "print(\"Vocabulary size is:\", VOCAB_SIZE)\n",
    "print(\"Vocabulary is:\", VOCAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data\n",
    "Inputs: are arbitrary sequences of length MAX_SEQ_LEN samples from the dataset\n",
    "Targets: are the same sequences shifted by one character to the right ie: one character in the future\n",
    "'''\n",
    "class CharacterDataset(Dataset):\n",
    "    def __init__(self, dataset_path:str, seq_len:int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.data = f.read().replace('\\n',' ').lower()\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Total possible number of samples is every\n",
    "        sequence length subset possible in the dataset\n",
    "        '''\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, i:int):\n",
    "        '''\n",
    "        Returns a randomly samples sequence of length seq_len (input)\n",
    "        and then a target sequence which is the same sequence but shifted \n",
    "        so it is one token ahead of the input sequence\n",
    "        The model therefore learns to predict the next character\n",
    "        '''\n",
    "        x = self.data[i:self.seq_len+i]\n",
    "        y = self.data[i+1:self.seq_len+i+1]\n",
    "        # Tokenize\n",
    "        x = encode_sequence([i for i in x])\n",
    "        y = encode_sequence([i for i in y])\n",
    "        return {\n",
    "            'inputs':torch.tensor(x),\n",
    "            'targets':torch.tensor(y),\n",
    "        }\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, dataset_path:str, seq_len:int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.data = f.read().replace(\"\\n\", \" \").translate(str.maketrans(\"\", \"\", string.punctuation)).lower().split(\" \")\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Total possible number of samples is every\n",
    "        sequence length subset possible in the dataset\n",
    "        '''\n",
    "        return len(self.data) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, i:int):\n",
    "        '''\n",
    "        Returns a randomly samples sequence of length seq_len (input)\n",
    "        and then a target sequence which is the same sequence but shifted \n",
    "        so it is one token ahead of the input sequence\n",
    "        The model therefore learns to predict the next character\n",
    "        '''\n",
    "        x = self.data[i:self.seq_len+i]\n",
    "        y = self.data[i+1:self.seq_len+i+1]\n",
    "        # Tokenize\n",
    "        x = encode_sequence(x)\n",
    "        y = encode_sequence(y)\n",
    "        return {\n",
    "            'inputs':torch.tensor(x),\n",
    "            'targets':torch.tensor(y),\n",
    "        }\n",
    "\n",
    "def load_dataloader(dataset_path: str, seq_len: int, batch_size:int, character_based:bool=True):\n",
    "    if character_based:\n",
    "        dataset = CharacterDataset(dataset_path, seq_len)\n",
    "    else:\n",
    "        dataset = WordDataset(dataset_path, seq_len)\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "def train_test_split(dataset_path:str, train_size:float):\n",
    "    assert train_size <= 1., 'Train size cannot be larger than 1'\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        dataset = f.read()\n",
    "    train = dataset[:int(train_size*len(dataset))]\n",
    "    test = dataset[int(train_size*len(dataset)):]\n",
    "    dataset_filename = dataset_path.split(\"/\")[-1]\n",
    "    dataset_root_path = \"/\".join(dataset_path.split(\"/\")[:-1])\n",
    "    with open(f'{dataset_root_path}/train_{dataset_filename}', 'w') as f:\n",
    "        f.write(train)\n",
    "    with open(f'{dataset_root_path}/test_{dataset_filename}', 'w') as f:\n",
    "        f.write(test)\n",
    "    print(f'Dataset split and written to: {dataset_root_path}/test_{dataset_filename} & {dataset_root_path}/train_{dataset_filename}')\n",
    "    return f'{dataset_root_path}/train_{dataset_filename}', f'{dataset_root_path}/test_{dataset_filename}'\n",
    "\n",
    "def test_dataloader():\n",
    "    train, test = train_test_split(DATASET_PATH, 0.9)\n",
    "    train_dataset = load_dataloader(train, MAX_SEQ_LEN, 1)\n",
    "    test_dataset = load_dataloader(test, MAX_SEQ_LEN, 1)\n",
    "    batch = next(iter(train_dataset))\n",
    "    x, y = batch['inputs'], batch['targets']\n",
    "    assert torch.all(x[:,1:].eq(y[:,:-1])), 'Target values are not right shifted by one token!'\n",
    "    print('Dataset is correct!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Transformer architecture: https://arxiv.org/abs/1706.03762\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_heads, vocab_size, emb_dim, n_blocks) -> None:\n",
    "        super().__init__()\n",
    "        self.text_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[EncoderBlock(emb_dim, n_heads) for i in range(n_blocks)]\n",
    "        )\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(emb_dim, n_heads) for i in range(n_blocks)])\n",
    "        self.out_fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        enc_x = self.text_embedding(encoder_input)\n",
    "        dec_x = self.text_embedding(decoder_input)\n",
    "        enc_x = enc_x + self.positional_embedding(encoder_input)\n",
    "        dec_x = dec_x + self.positional_embedding(decoder_input)\n",
    "        encoder_output = self.encoder(enc_x)\n",
    "        for block in self.decoder:\n",
    "            dec_x = block(encoder_output, dec_x)\n",
    "        dec_x = self.out_fc(dec_x)\n",
    "        return dec_x\n",
    "\n",
    "    def generate(self, prompt:str, max_len:int):\n",
    "        '''\n",
    "        Used for inference\n",
    "        '''\n",
    "        i = 0\n",
    "        output = torch.tensor([[encode_sequence('<START>')]])\n",
    "        while i < max_len:\n",
    "            generated_output = self.forward(prompt, output)\n",
    "            # Select the most probable token\n",
    "            output = torch.cat((output, generated_output.argmax(dim=-1).unsqueeze(0)), dim=1)\n",
    "            i += 1\n",
    "        return output\n",
    "        \n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, in_d: int, out_d: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Parralelise the attention heads \n",
    "        self.W_q = nn.Linear(in_d, in_d*n_heads, bias=False)\n",
    "        self.W_k = nn.Linear(in_d, in_d*n_heads, bias=False)\n",
    "        self.W_v = nn.Linear(in_d, in_d*n_heads, bias=False)\n",
    "        self.fc_out = nn.Linear(in_d*n_heads, out_d, bias=True)\n",
    "\n",
    "    def softmax_attention(self, q, k, v, mask=None):\n",
    "        # Caculate dot product\n",
    "        qk = q @ k.transpose(-2, -1)\n",
    "        # Normalise by the square root of the dimension\n",
    "        qk = qk / math.sqrt(q.shape[-1])\n",
    "        # Apply mask if decoder\n",
    "        if mask is not None:\n",
    "            qk = qk.masked_fill(mask == 0, -1e10)\n",
    "        # Calculate attention scores\n",
    "        qk = F.softmax(qk, dim=-1)\n",
    "        # Apply attention scores to values\n",
    "        out = qk @ v\n",
    "        return out\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # [batch_size, seq_len, emb_dim] -> [batch_size, n_heads, seq_len, emb_dim/n_heads]\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(\n",
    "            batch_size, -1, self.n_heads, x.shape[-1] // self.n_heads\n",
    "        ).permute(0, 2, 1, 3).reshape(batch_size * self.n_heads, -1, x.shape[-1] // self.n_heads)\n",
    "\n",
    "    def concat_heads(self, x):\n",
    "        # [batch_size * n_heads, seq_len, emb_dim/n_heads] -> [batch_size, seq_len, emb_dim*heads]\n",
    "        batch_size = x.shape[0] // self.n_heads\n",
    "        return x.reshape(batch_size, self.n_heads, -1, x.shape[-1]).permute(0, 2, 1, 3).reshape(batch_size, -1, x.shape[-1] * self.n_heads)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Embed the input [batch_size, seq_len, emb_dim] -> [batch_size, seq_len, n_heads, emb_dim/n_heads]\n",
    "        q = self.split_heads(self.W_q(q))\n",
    "        k = self.split_heads(self.W_k(k))\n",
    "        v = self.split_heads(self.W_v(v))\n",
    "        # Calculate attention\n",
    "        attention_score = self.softmax_attention(q, k, v, mask)\n",
    "        # Concatenate the heads\n",
    "        out = self.concat_heads(attention_score)\n",
    "        # Apply final linear layer\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, in_d, out_d) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_d, out_d),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_d, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadedAttention(n_heads, in_d, in_d)\n",
    "        # Note: FF block is sandwhiched between layer norms as res conenction occurs here\n",
    "        self.ff = (FeedForwardBlock(in_d, in_d))\n",
    "        self.norm_1 = nn.LayerNorm(in_d)\n",
    "        self.norm_2 = nn.LayerNorm(in_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.mha(x,x,x)\n",
    "        x = self.norm_1(x + res)\n",
    "        res = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm_2(x + res)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_d, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        # Decoder only\n",
    "        self.masked_mha = MultiHeadedAttention(n_heads, in_d, in_d)\n",
    "        # For encoder and decoder\n",
    "        self.cross_mha = MultiHeadedAttention(n_heads, in_d, in_d)\n",
    "        # Note: FF block is sandwhiched between layer norms as res conenction occurs here\n",
    "        self.ff = (FeedForwardBlock(in_d, in_d))\n",
    "        self.norm_1 = nn.LayerNorm(in_d)\n",
    "        self.norm_2 = nn.LayerNorm(in_d)\n",
    "\n",
    "    def make_mask(self, dec_x):\n",
    "        # Create a mask for the decoder to prevent it from looking ahead\n",
    "        mask = torch.triu(torch.ones(dec_x.shape[1], dec_x.shape[1]), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        return mask.unsqueeze(0).to(dec_x.device)\n",
    "\n",
    "    def forward(self, enc_x, dec_x):\n",
    "        # Layer 1 \n",
    "        res = dec_x\n",
    "        mask = self.make_mask(dec_x)\n",
    "        dec_x = self.masked_mha(dec_x, dec_x, dec_x, mask)\n",
    "        dec_x = self.norm_1(dec_x + res)\n",
    "        # Layer 2\n",
    "        res = dec_x\n",
    "        x = self.cross_mha(dec_x, enc_x, enc_x)\n",
    "        x = self.norm_1(x + res)\n",
    "        # Layer 3\n",
    "        res = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm_2(x + res)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_dl: DataLoader,\n",
    "        val_dl: DataLoader,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        character_based: bool = False,\n",
    "    ) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.character_based = character_based\n",
    "\n",
    "    def run(self, epochs: int):\n",
    "        self.model.train()\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch: {epoch}')\n",
    "            train_loss = self.train_step()\n",
    "            val_loss, output = self.val_step()\n",
    "            print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "            # Sample output\n",
    "            if self.character_based:\n",
    "                print(f'Sample output: {decode_character_sequence(output[0].argmax(dim=-1))}')\n",
    "            else:\n",
    "                print(f'Sample output: {decode_word_sequence(output[0].argmax(dim=-1))}')\n",
    "\n",
    "        # Save model\n",
    "        torch.save(self.model.state_dict(), 'model.pt')\n",
    "\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Performs one pass over the training data\n",
    "        \"\"\"\n",
    "        train_loss = 0\n",
    "        self.model.train()\n",
    "        for batch in tqdm(self.train_dl, desc='Train Step', leave=False):\n",
    "            x, y = batch['inputs'], batch['targets']\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(x, y)\n",
    "            loss: torch.Tensor = self.loss_fn(out.permute(0,2,1), y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(self.train_dl)\n",
    "        return train_loss\n",
    "\n",
    "    def val_step(self):\n",
    "        \"\"\"\n",
    "        Performs one pass over the validation set\n",
    "        \"\"\"\n",
    "        val_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_dl, desc='Val Step', leave=False):\n",
    "                x, y = batch['inputs'], batch['targets']\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                out = self.model(x, y)\n",
    "                loss: torch.Tensor = self.loss_fn(out.permute(0,2,1), y)\n",
    "                val_loss += loss.item()\n",
    "                return val_loss, out\n",
    "            val_loss /= len(self.val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 20 epochs...\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 2.315573496431918, Val Loss: 1.6418728828430176\n",
      "Sample output: theresweinitofaboutseetheresnotonetocouldallusandthatisalldecidingnoornotlifeislotlivingistoanswerthetheresquestioninseeallothermatterseefromthatsobasicallytobeornottobedoyouthinktheresatruthtothatseethisquestionofwhyliveatalldoyouthinktherestruthtothatseebeingareallyreallyimportantquestionforustoansweryesyeahandithinkitsinterestingthatihisinabookthethefirstseewasandwhichidontknowifhehadtendtoallidontwellthetendaboutitbutitsabouthimasachildrightandits\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.4514031931355193, Val Loss: 0.7264035940170288\n",
      "Sample output: observingabsolutelyinrightofabouttendtheresleftonetothisphilosophicalusandthatistenddecidingsomeoneornotlifeisoutsidelivingistoanswerthecantquestioninphilosophyallothersomeonejustfromthatsobasicallytobeornottobedoyouthinktheresatruthtothatstatementthisquestionofwhyliveatalldoyouthinktherestruthtothatstatementbeingareallyreallyimportantquestionforustoansweryesyeahandithinkitsinterestingthatafterhisdeathabookcalledthefirstmanwaspublishedwhichidontknowifhehadintendedtopublishidontrememberthespecificsaboutitbutitsabouthimasachildrightandits\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.0507705014299702, Val Loss: 0.404191792011261\n",
      "Sample output: observingweinshouldofpersonsymphonytheresnotonetyingseriousphilosophicalusandthatissuicidedecidingquestionornotlifeisworthlivingistoanswerthefundamentalquestioninphilosophyallotherquestionsfollowfromthatsobasicallytobeornottobedoyouthinktheresatruthtothatstatementthisquestionofwhyliveatalldoyouthinktherestruthtothatstatementbeingareallyreallyimportantquestionforustoansweryesyeahandithinkitsinterestingthatafterhisdeathabookcalledthefirstmanwaspublishedwhichidontknowifhehadintendedtopublishidontrememberthespecificsaboutitbutitsabouthimasachildrightandits\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 0.016924687288701536, Val Loss: 0.23815235495567322\n",
      "Sample output: observinghisinshouldofaboutdiscoveredtheresonlyonerealseriousphilosophicalproblemandthatissuicidedecidingwhetherornotlifeisworthlivingistoanswerthefundamentalquestioninphilosophyallotherquestionsfollowfromthatsobasicallytobeornottobedoyouthinktheresatruthtothatstatementthisquestionofwhyliveatalldoyouthinktherestruthtothatstatementbeingareallyreallyimportantquestionforustoansweryesyeahandithinkitsinterestingthatafterhisdeathabookcalledthefirstmanwaspublishedwhichidontknowifhehadintendedtopublishidontrememberthespecificsaboutitbutitsabouthimasachildrightandits\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.008707570611826471, Val Loss: 0.14447733759880066\n",
      "Sample output: nihilismhisinwhereofsisyphusquotetheresonlyonerealseriousphilosophicalproblemandthatissuicidedecidingwhetherornotlifeisworthlivingistoanswerthefundamentalquestioninphilosophyallotherquestionsfollowfromthatsobasicallytobeornottobedoyouthinktheresatruthtothatstatementthisquestionofwhyliveatalldoyouthinktherestruthtothatstatementbeingareallyreallyimportantquestionforustoansweryesyeahandithinkitsinterestingthatafterhisdeathabookcalledthefirstmanwaspublishedwhichidontknowifhehadintendedtopublishidontrememberthespecificsaboutitbutitsabouthimasachildrightandits\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 0.00525977748974755, Val Loss: 0.09895338118076324\n",
      "Sample output: nihilismhisinmythofsisyphusquotetheresonlyonerealseriousphilosophicalproblemandthatissuicidedecidingwhetherornotlifeisworthlivingistoanswerthefundamentalquestioninphilosophyallotherquestionsfollowfromthatsobasicallytobeornottobedoyouthinktheresatruthtothatstatementthisquestionofwhyliveatalldoyouthinktherestruthtothatstatementbeingareallyreallyimportantquestionforustoansweryesyeahandithinkitsinterestingthatafterhisdeathabookcalledthefirstmanwaspublishedwhichidontknowifhehadintendedtopublishidontrememberthespecificsaboutitbutitsabouthimasachildrightandits\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 0.0036652234999613987, Val Loss: 0.04821851849555969\n",
      "Sample output: enhisinmythofsisyphusquotetheresonlyonerealseriousphilosophicalproblemandthatissuicidedecidingwhetherornotlifeisworthlivingistoanswerthefundamentalquestioninphilosophyallotherquestionsfollowfromthatsobasicallytobeornottobedoyouthinktheresatruthtothatstatementthisquestionofwhyliveatalldoyouthinktherestruthtothatstatementbeingareallyreallyimportantquestionforustoansweryesyeahandithinkitsinterestingthatafterhisdeathabookcalledthefirstmanwaspublishedwhichidontknowifhehadintendedtopublishidontrememberthespecificsaboutitbutitsabouthimasachildrightandits\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_dl, val_dl \u001b[39m=\u001b[39m load_dataloader(\u001b[39m'\u001b[39m\u001b[39m./test.wav.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m8\u001b[39m, character_based\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), load_dataloader(\u001b[39m'\u001b[39m\u001b[39m./test.wav.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m8\u001b[39m, character_based\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m Trainer(model, train_dl, val_dl, loss_fn, optimizer)\u001b[39m.\u001b[39;49mrun(epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[1;32m     24\u001b[0m     val_loss, output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_step()\n\u001b[1;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 45\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     loss: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(out\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m), y)\n\u001b[1;32m     44\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     46\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     47\u001b[0m train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dl)\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/picoGPT/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/picoGPT/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/picoGPT/venv/lib/python3.8/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    163\u001b[0m           grads,\n\u001b[1;32m    164\u001b[0m           exp_avgs,\n\u001b[1;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    167\u001b[0m           state_steps,\n\u001b[1;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/picoGPT/venv/lib/python3.8/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m func(params,\n\u001b[1;32m    220\u001b[0m      grads,\n\u001b[1;32m    221\u001b[0m      exp_avgs,\n\u001b[1;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    224\u001b[0m      state_steps,\n\u001b[1;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/Desktop/Programming/DeepLearning/picoGPT/venv/lib/python3.8/site-packages/torch/optim/adamw.py:274\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    273\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 274\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[1;32m    277\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(n_heads=4, vocab_size=len(VOCAB), emb_dim=512, n_blocks=6)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "train_dl, val_dl = load_dataloader('./test.wav.txt', 128, 8, character_based=True), load_dataloader('./test.wav.txt', 128, 8, character_based=True)\n",
    "Trainer(model, train_dl, val_dl, loss_fn, optimizer).run(epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model: Transformer, pth_path:str) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model = model\n",
    "        self.model.load_state_dict(torch.load(pth_path))\n",
    "\n",
    "    def predict(self, prompt:str):\n",
    "        return self.model.generate(prompt)\n",
    "\n",
    "    def launch_interactive(self):\n",
    "        while True:\n",
    "            prompt = input(\"Enter a prompt: \")\n",
    "            print(self.predict(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66e58e144739996868149dbe6413ac7a12839919439ff5ebb9e75c32680b3c30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
